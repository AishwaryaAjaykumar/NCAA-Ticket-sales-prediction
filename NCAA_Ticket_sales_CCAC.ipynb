{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1thS8l9AP3aK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e229f8db-4aba-44bc-e009-d379b62b5a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('display.max_columns', None)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import re\n",
        "# !pip install scikeras\n",
        "# from scikeras.wrappers import KerasClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the directory where the data is stored\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/ccac_data/dev_cleaned_train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/ccac_data/dev_cleaned_test.csv')"
      ],
      "metadata": {
        "id": "xO147PZrTV6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eimzQJSoXud5"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vOErwlNaAuU",
        "outputId": "ab79f920-61bc-494e-d9ca-09d81b71ee79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'HasCustomerClickedOrOpenedEmailsSixMonthsPrior': 116138 replacements made.\n",
            "Column 'CustomerCity': 13649 replacements made.\n",
            "Column 'CustomerState': 13620 replacements made.\n",
            "Column 'CustomerZipCode': 13588 replacements made.\n",
            "Column 'CustomerInstitutionAffinity': 20849 replacements made.\n",
            "Column 'HasCustomerClickedOrOpenedEmailsSixMonthsPrior': 11602 replacements made.\n",
            "Column 'CustomerFirstWBBActionDate': 1102 replacements made.\n",
            "Column 'CustomerFirstWBBPurchaseDate': 3979 replacements made.\n",
            "Column 'CustomerLastWBBActionDate': 1102 replacements made.\n",
            "Column 'CustomerLastWBBPurchaseDate': 3979 replacements made.\n",
            "Column 'EventRoundName': 19118 replacements made.\n",
            "Column 'IsEventFinalSite': 19118 replacements made.\n",
            "Column 'EventSession': 19118 replacements made.\n",
            "Column 'EventBeginDate': 19118 replacements made.\n",
            "Column 'EventEndDate': 19118 replacements made.\n",
            "Column 'HostingInstitution': 19118 replacements made.\n",
            "Column 'FacilityName': 19241 replacements made.\n",
            "Column 'FacilityDescription': 19241 replacements made.\n",
            "Column 'FacilityCity': 19241 replacements made.\n",
            "Column 'FacilityState': 19241 replacements made.\n",
            "Column 'FacilityZipCode': 19313 replacements made.\n",
            "Column 'Country': 4 replacements made.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def replace_with_nan_and_count(df):\n",
        "    \"\"\"\n",
        "    This function takes a pandas DataFrame, replaces all occurrences of\n",
        "    specified placeholders with NaN, and prints the count of replacements\n",
        "    for each column.\n",
        "\n",
        "    :param df: pandas DataFrame\n",
        "    :return: DataFrame with the specified values replaced with NaN, and prints counts\n",
        "    \"\"\"\n",
        "    # Define the values you want to replace with NaN\n",
        "    replace_values = ['(UNK)','UNK', '(N/A)', 'NULL', '(NULL)','nan','unk','(unk)','na','null','(null)']\n",
        "    replacement_dict = {value: np.nan for value in replace_values}\n",
        "\n",
        "    # Track and print the count of replacements for each column\n",
        "    for column in df.columns:\n",
        "        original_non_nan = df[column].notna().sum()  # Count non-NaN before replacement\n",
        "        df[column].replace(replacement_dict, inplace=True)\n",
        "        new_non_nan = df[column].notna().sum()  # Count non-NaN after replacement\n",
        "        replacements = original_non_nan - new_non_nan  # The difference is the count of replacements\n",
        "\n",
        "        if replacements > 0:\n",
        "            print(f\"Column '{column}': {replacements} replacements made.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df = replace_with_nan_and_count(train_df)\n",
        "df_test = replace_with_nan_and_count(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dOe2Cva5iaZ"
      },
      "outputs": [],
      "source": [
        "def replace_year_with_common_date(df, columns):\n",
        "    for column in columns:\n",
        "        # Convert column to datetime, errors='coerce' will turn the 'year-only' values into NaT\n",
        "        df[column] = pd.to_datetime(df[column], errors='coerce', format='%m/%d/%y')\n",
        "\n",
        "        # Find rows where the column is NaT (originally year-only values)\n",
        "        year_only_rows = df[column].isna()\n",
        "\n",
        "        # Extract the year from these rows from the original DataFrame\n",
        "        years = pd.to_datetime(df.loc[year_only_rows, column], format='%Y').dt.year\n",
        "\n",
        "        # For each unique year, find the most common date and replace the year-only values\n",
        "        for year in years.unique():\n",
        "            if pd.isna(year):  # Skip if year is NaT\n",
        "                continue\n",
        "            # Filter rows with the same year and find the most common date\n",
        "            common_date = df[df[column].dt.year == year][column].mode()\n",
        "            if not common_date.empty:\n",
        "                # Replace year-only values with the most common date for that year\n",
        "                df.loc[year_only_rows & (df[column].dt.year == year), column] = common_date.iloc[0]\n",
        "            else:\n",
        "                # If no common date is found, optionally handle this case (e.g., replace with a default date)\n",
        "                pass\n",
        "\n",
        "        # Convert back to original string format if needed\n",
        "        df[column] = df[column].dt.strftime('%m/%d/%y')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "columns_to_process = ['CustomerFirstWBBActionDate', 'CustomerFirstWBBPurchaseDate', 'CustomerLastWBBActionDate', 'CustomerLastWBBPurchaseDate', 'EventBeginDate', 'EventEndDate']\n",
        "df = replace_year_with_common_date(df, columns_to_process)\n",
        "df_test = replace_year_with_common_date(df_test, columns_to_process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQarRMrFboBe"
      },
      "outputs": [],
      "source": [
        "def values_to_lowercase(df, columns=['CustomerState', 'CustomerCity']):\n",
        "    \"\"\"\n",
        "    Convert values in specified columns of a DataFrame to lowercase.\n",
        "    If no columns are specified, default columns are converted to lowercase.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame\n",
        "    - columns: List of column names whose values are to be converted to lowercase.\n",
        "               Defaults to ['CustomerState', 'CustomerCity'] if not specified.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with values in specified columns converted to lowercase.\n",
        "    \"\"\"\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            # Convert column values to lowercase\n",
        "            df[col] = df[col].str.lower()\n",
        "        else:\n",
        "            print(f\"Column '{col}' not found in DataFrame.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "df = values_to_lowercase(df, columns=['CustomerState', 'CustomerCity'])\n",
        "df_test=values_to_lowercase(df_test, columns=['CustomerState', 'CustomerCity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_MlYjTJfF8C",
        "outputId": "ebc4afd3-3f2f-419f-b0ed-483c6ef4036c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correcting 'nan' to 'nan'\n",
            "Correcting 'a' to 'il'\n",
            "Correcting 'nan' to 'nan'\n"
          ]
        }
      ],
      "source": [
        "def standardize_customer_state_with_print(df):\n",
        "    \"\"\"\n",
        "    Standardize the 'CustomerState' column values in the DataFrame to a uniform format\n",
        "    and print the original and replacement values. Specific unwanted values are dropped.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame with a 'CustomerState' column.\n",
        "\n",
        "    Returns:\n",
        "    - Modified DataFrame with standardized 'CustomerState' values.\n",
        "    \"\"\"\n",
        "\n",
        "    # List of specific unwanted values to drop\n",
        "    values_to_drop = ['ste', '19th', 'n', '14']\n",
        "\n",
        "    # Drop specific unwanted values\n",
        "    df = df[~df['CustomerState'].isin(values_to_drop)]\n",
        "\n",
        "    # Dictionary to map non-standard to standard state abbreviations\n",
        "    state_corrections = {\n",
        "        'newjersey': 'nj',\n",
        "        'newyork': 'ny',\n",
        "        'southcarolina': 'sc',\n",
        "        'northdakota': 'nd',\n",
        "        'southdakota': 'sd',\n",
        "        'northcarolina': 'nc',\n",
        "        'newhampshire': 'nh',\n",
        "        'britishcolumbia': 'bc',\n",
        "        'pensylvania': 'pa',  # Assuming this is a misspelling of 'Pennsylvania'\n",
        "        'pensylvania': 'pa',\n",
        "        'maryl': 'md',\n",
        "        'hyattsville': 'md',\n",
        "        'a': 'il',\n",
        "        'north': 'nc'\n",
        "        # Add more mappings as needed\n",
        "    }\n",
        "\n",
        "    # Apply corrections\n",
        "    for state in df['CustomerState'].unique():\n",
        "        corrected_state = state_corrections.get(state, state)\n",
        "        if state != corrected_state:\n",
        "            print(f\"Correcting '{state}' to '{corrected_state}'\")\n",
        "            df['CustomerState'] = df['CustomerState'].replace(state, corrected_state)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = standardize_customer_state_with_print(df)\n",
        "df_test = standardize_customer_state_with_print(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_state_to_country(state):\n",
        "    state_to_country = {\n",
        "        'alberta': 'Canada',\n",
        "        'manitoba': 'Canada',\n",
        "        'qc': 'Canada',\n",
        "        'n5p4j9': 'Canada',\n",
        "        'zuidholland': 'Holland',\n",
        "        'istanbul': 'Turkey',\n",
        "        'tokyo': 'Japan',\n",
        "        'kennington': 'England',\n",
        "        'jal': 'Mexico',\n",
        "        'wlkp': 'Other',\n",
        "        'vic': 'Spain',\n",
        "        'Unknown': 'Unknown'\n",
        "    }\n",
        "    return state_to_country.get(state.lower(), 'USA')\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df['CustomerCountry'] = df['CustomerState'].apply(map_state_to_country)\n",
        "df_test['CustomerCountry'] = df_test['CustomerState'].apply(map_state_to_country)"
      ],
      "metadata": {
        "id": "0hdm8Lm5MKAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC4iKkoKcWKD",
        "outputId": "9b5bced7-7d8d-499c-a01f-44cb38548776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'Unnamed: 0': 0 changes made.\n",
            "Column 'RecordID': 0 changes made.\n",
            "Column 'ChampionshipYear': 0 changes made.\n",
            "Column 'CustomerID': 0 changes made.\n",
            "Column 'CustomerCity': 167 changes made.\n",
            "Column 'CustomerState': 0 changes made.\n",
            "Column 'CustomerZipCode': 18 changes made.\n",
            "Column 'CustomerInstitutionAffinity': 172 changes made.\n",
            "Column 'IsCustomerInNCAAMembership': 0 changes made.\n",
            "Column 'HasCustomerClickedOrOpenedEmailsSixMonthsPrior': 0 changes made.\n",
            "Column 'CustomerFirstWBBActionDate': 198193 changes made.\n",
            "Column 'CustomerFirstWBBPurchaseDate': 169959 changes made.\n",
            "Column 'CustomerLastWBBActionDate': 198193 changes made.\n",
            "Column 'CustomerLastWBBPurchaseDate': 169959 changes made.\n",
            "Column 'EventRoundName': 0 changes made.\n",
            "Column 'IsEventFinalSite': 0 changes made.\n",
            "Column 'EventSession': 9420 changes made.\n",
            "Column 'EventBeginDate': 17955 changes made.\n",
            "Column 'EventEndDate': 17955 changes made.\n",
            "Column 'HostingInstitution': 4549 changes made.\n",
            "Column 'FacilityName': 1967 changes made.\n",
            "Column 'FacilityDescription': 0 changes made.\n",
            "Column 'FacilityCity': 0 changes made.\n",
            "Column 'FacilityState': 0 changes made.\n",
            "Column 'FacilityZipCode': 16004 changes made.\n",
            "Column 'ActivityType': 0 changes made.\n",
            "Column 'Country': 0 changes made.\n",
            "Column 'Unnamed: 0': 0 changes made.\n",
            "Column 'RecordID': 0 changes made.\n",
            "Column 'ChampionshipYear': 0 changes made.\n",
            "Column 'CustomerID': 0 changes made.\n",
            "Column 'CustomerCity': 29 changes made.\n",
            "Column 'CustomerState': 0 changes made.\n",
            "Column 'CustomerZipCode': 2 changes made.\n",
            "Column 'CustomerInstitutionAffinity': 20 changes made.\n",
            "Column 'IsCustomerInNCAAMembership': 0 changes made.\n",
            "Column 'HasCustomerClickedOrOpenedEmailsSixMonthsPrior': 0 changes made.\n",
            "Column 'CustomerFirstWBBActionDate': 19833 changes made.\n",
            "Column 'CustomerFirstWBBPurchaseDate': 16956 changes made.\n",
            "Column 'CustomerLastWBBActionDate': 19833 changes made.\n",
            "Column 'CustomerLastWBBPurchaseDate': 16956 changes made.\n",
            "Column 'EventRoundName': 0 changes made.\n",
            "Column 'IsEventFinalSite': 0 changes made.\n",
            "Column 'EventSession': 931 changes made.\n",
            "Column 'EventBeginDate': 1817 changes made.\n",
            "Column 'EventEndDate': 1817 changes made.\n",
            "Column 'HostingInstitution': 445 changes made.\n",
            "Column 'FacilityName': 217 changes made.\n",
            "Column 'FacilityDescription': 0 changes made.\n",
            "Column 'FacilityCity': 0 changes made.\n",
            "Column 'FacilityState': 0 changes made.\n",
            "Column 'FacilityZipCode': 0 changes made.\n",
            "Column 'Country': 0 changes made.\n"
          ]
        }
      ],
      "source": [
        "def remove_special_characters_and_count_all(df):\n",
        "    \"\"\"\n",
        "    Remove all special characters from values in all columns of a DataFrame\n",
        "    and print the count of changes for each column without changing column names.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with special characters removed from values in all columns.\n",
        "    \"\"\"\n",
        "    change_counts = {}  # Initialize change count dictionary\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Convert all columns to string to ensure the regex can be applied\n",
        "        original_values = df[col].astype(str)\n",
        "        cleaned_values = original_values.apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "        # Count changes (ignoring changes that are solely due to conversion to string)\n",
        "        change_counts[col] = (original_values != cleaned_values).sum()\n",
        "\n",
        "        # Apply changes, converting back to original dtype if possible\n",
        "        cleaned_series = cleaned_values.apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "        try:\n",
        "            df[col] = pd.to_numeric(cleaned_series, errors='ignore')\n",
        "        except ValueError:\n",
        "            df[col] = cleaned_series\n",
        "\n",
        "    # Print change counts\n",
        "    for col, count in change_counts.items():\n",
        "        print(f\"Column '{col}': {count} changes made.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Remove special characters and get change counts for all columns\n",
        "df = remove_special_characters_and_count_all(df)\n",
        "df_test = remove_special_characters_and_count_all(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-8zCXCCwsm8"
      },
      "outputs": [],
      "source": [
        "  def fill_categorical_nans(df):\n",
        "      \"\"\"\n",
        "      Fill NaN, NULL, and variations with 'unknown' in all categorical columns of the DataFrame.\n",
        "\n",
        "      Parameters:\n",
        "      - df: pandas DataFrame.\n",
        "\n",
        "      Returns:\n",
        "      - DataFrame with 'unknown' filled in for NaNs in categorical columns.\n",
        "      \"\"\"\n",
        "\n",
        "      # Define variations of null values that should be considered\n",
        "      null_variations = ['NaN', 'NULL', 'null', '(null)', 'N/A', 'n/a', 'na', '-', '']\n",
        "\n",
        "      # Replace each null variation with 'unknown' in object dtype columns\n",
        "      for col in df.columns:\n",
        "          df[col] = df[col].apply(lambda x: 'Unknown' if str(x).strip() in null_variations else x)\n",
        "\n",
        "      return df\n",
        "\n",
        "  # Fill NaN and variations in categorical columns with 'unknown'\n",
        "  df = fill_categorical_nans(df)\n",
        "  df_test = fill_categorical_nans(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9Y92nxYi8-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GVoY_MpjCgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_csv('/content/drive/MyDrive/ccac_data/train_clean.csv', index=False)\n",
        "# df_test.to_csv('/content/drive/MyDrive/ccac_data/test_clean.csv', index=False)"
      ],
      "metadata": {
        "id": "4yGxwKdNC7pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyv0CBVAVYat"
      },
      "source": [
        "# ensemble 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwjs596eVfVq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Assuming 'df' and 'df_test' are already loaded with your data\n",
        "\n",
        "# Define preprocessing\n",
        "categorical_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['ActivityType'])\n",
        "X_preprocessed = preprocessor.fit_transform(df.drop('ActivityType', axis=1))\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define individual estimators\n",
        "estimator1 = ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "estimator2 = ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
        "estimator3 = ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'))\n",
        "\n",
        "# Create a VotingClassifier with soft voting\n",
        "voting_clf = VotingClassifier(estimators=[estimator1, estimator2, estimator3], voting='soft')\n",
        "\n",
        "# Create a pipeline with SMOTE and the VotingClassifier\n",
        "ensemble_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('voting', voting_clf)\n",
        "])\n",
        "\n",
        "# Define a parameter grid to search over (simplified for demonstration)\n",
        "param_grid = {\n",
        "    'voting__rf__max_depth': [10, None],\n",
        "    'voting__gb__learning_rate': [0.01, 0.1],\n",
        "    # Add more parameters as needed\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "grid_search = GridSearchCV(ensemble_pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Print the best parameters and the corresponding score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best accuracy found: \", grid_search.best_score_)\n",
        "\n",
        "# Predict and evaluate using the best found parameters\n",
        "y_pred_val_encoded = grid_search.predict(X_val)\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val_encoded)}')\n",
        "print(classification_report(y_val_encoded, y_pred_val_encoded))\n",
        "\n",
        "# Predict on the test data\n",
        "X_test_preprocessed = preprocessor.transform(df_test)  # Ensure df_test is preprocessed similarly\n",
        "test_predictions_encoded = grid_search.predict(X_test_preprocessed)\n",
        "df_test['ActivityType'] = label_encoder.inverse_transform(test_predictions_encoded)\n",
        "\n",
        "# Output the prediction in the desired format\n",
        "output_df = df_test[['RecordID', 'ActivityType']]\n",
        "output_df.to_csv('/content/drive/MyDrive/ccac_data/ensemble_predictions_1.csv', index=False)\n",
        "print(output_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtCzcofRVquw"
      },
      "outputs": [],
      "source": [
        "print(f'Precision: {precision_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Recall: {recall_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'F1 Score: {f1_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, pipeline.predict(X_train))}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val_encoded)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4GuVEyrcvbN",
        "outputId": "e71727c2-0e5b-4b14-da21-3ac656cb0b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.5780253768667726\n",
            "Recall: 0.6226365308334768\n",
            "F1 Score: 0.5825139217005352\n",
            "Train Accuracy: 0.9959081514402113\n",
            "Validation Accuracy: 0.9829159896779126\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.44      0.38       142\n",
            "           1       1.00      1.00      1.00     38237\n",
            "           2       0.22      0.11      0.14        19\n",
            "           3       0.97      0.87      0.92      2612\n",
            "           4       0.28      0.66      0.40       171\n",
            "           5       0.66      0.65      0.66       671\n",
            "\n",
            "    accuracy                           0.98     41852\n",
            "   macro avg       0.58      0.62      0.58     41852\n",
            "weighted avg       0.99      0.98      0.98     41852\n",
            "\n",
            "   RecordID ActivityType\n",
            "0     26355  No Activity\n",
            "1     26641  No Activity\n",
            "2     26836  No Activity\n",
            "3     26900  No Activity\n",
            "4     26948  No Activity\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Define preprocessing\n",
        "categorical_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['ActivityType'])\n",
        "X_preprocessed = preprocessor.fit_transform(df.drop('ActivityType', axis=1))\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define individual estimators\n",
        "estimator1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "estimator2 = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "estimator3 = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Create a VotingClassifier with soft voting\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', estimator1),\n",
        "    ('gb', estimator2),\n",
        "    ('xgb', estimator3)],\n",
        "    voting='soft')\n",
        "\n",
        "# Create a pipeline with SMOTE and the VotingClassifier\n",
        "ensemble_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('voting', voting_clf)\n",
        "])\n",
        "\n",
        "# Train the model on the training set\n",
        "ensemble_pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_val_encoded = ensemble_pipeline.predict(X_val)\n",
        "\n",
        "# Print precision, recall, f1-score, and accuracy for the validation set using the encoded labels\n",
        "print(f'Precision: {precision_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Recall: {recall_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'F1 Score: {f1_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, ensemble_pipeline.predict(X_train))}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val_encoded)}')\n",
        "print(classification_report(y_val_encoded, y_pred_val_encoded))\n",
        "\n",
        "# Predict on the test data using the trained pipeline\n",
        "X_test_preprocessed = preprocessor.transform(df_test)\n",
        "test_predictions_encoded = ensemble_pipeline.predict(X_test_preprocessed)\n",
        "df_test['ActivityType'] = label_encoder.inverse_transform(test_predictions_encoded)\n",
        "\n",
        "# Output the prediction in the desired format\n",
        "output_df = df_test[['RecordID', 'ActivityType']]\n",
        "output_df.to_csv('/content/drive/MyDrive/ccac_data/ensemble_predictions_1.csv', index=False)\n",
        "print(output_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQeyt-pGDDLx"
      },
      "source": [
        "# ensemble 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3agfzQzp3Zuv",
        "outputId": "8460cb76-59e7-492a-9e0a-0ddd9b22e05f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.5956815404980214\n",
            "Recall: 0.5006641849828212\n",
            "F1 Score: 0.5102475204569987\n",
            "Train Accuracy: 0.9947074776292367\n",
            "Validation Accuracy: 0.9868584535983943\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.16      0.24       142\n",
            "           1       1.00      1.00      1.00     38237\n",
            "           2       0.20      0.05      0.08        19\n",
            "           3       0.91      0.96      0.93      2612\n",
            "           4       0.35      0.05      0.08       171\n",
            "           5       0.67      0.78      0.72       671\n",
            "\n",
            "    accuracy                           0.99     41852\n",
            "   macro avg       0.60      0.50      0.51     41852\n",
            "weighted avg       0.98      0.99      0.98     41852\n",
            "\n",
            "   RecordID ActivityType\n",
            "0     26355  No Activity\n",
            "1     26641  No Activity\n",
            "2     26836  No Activity\n",
            "3     26900  No Activity\n",
            "4     26948  No Activity\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define preprocessing\n",
        "categorical_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['ActivityType'])\n",
        "X_preprocessed = preprocessor.fit_transform(df.drop('ActivityType', axis=1))\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute class weights for imbalance handling\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Update individual estimators with class_weight where applicable\n",
        "estimator1 = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=class_weights_dict)\n",
        "estimator2 = GradientBoostingClassifier(n_estimators=100, random_state=42)  # No class_weight parameter\n",
        "estimator3 = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "\n",
        "\n",
        "# Create a VotingClassifier with soft voting\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', estimator1),\n",
        "    ('gb', estimator2),\n",
        "    ('xgb', estimator3)],\n",
        "    voting='soft')\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Count the occurrences of each class in the target variable\n",
        "class_counts = Counter(y_train_encoded)\n",
        "\n",
        "# Example: Increase the sample size of minority classes with SMOTE\n",
        "# Decrease the sample size of the majority class with RandomUnderSampler\n",
        "# This is a placeholder; adjust according to your specific needs\n",
        "sampling_strategy_smote = {class_label: int(count * 1.5) for class_label, count in class_counts.items() if count / len(y_train_encoded) < 0.91}\n",
        "sampling_strategy_under = {class_label: int(count * 0.5) for class_label, count in class_counts.items() if count / len(y_train_encoded) >= 0.91}\n",
        "\n",
        "# Update the pipeline steps for SMOTE and RandomUnderSampler\n",
        "imbalance_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state=42, sampling_strategy=sampling_strategy_smote)),  # Adjust for multi-class\n",
        "    ('under', RandomUnderSampler(random_state=42, sampling_strategy=sampling_strategy_under)),  # Adjust for multi-class\n",
        "    ('voting', voting_clf)\n",
        "])\n",
        "\n",
        "\n",
        "# Train the model on the training set\n",
        "imbalance_pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_val_encoded = imbalance_pipeline.predict(X_val)\n",
        "\n",
        "# Print evaluation metrics for the validation set\n",
        "print(f'Precision: {precision_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Recall: {recall_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'F1 Score: {f1_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, imbalance_pipeline.predict(X_train))}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val_encoded)}')\n",
        "print(classification_report(y_val_encoded, y_pred_val_encoded))\n",
        "\n",
        "# Predict on the test data using the trained pipeline\n",
        "X_test_preprocessed = preprocessor.transform(df_test)\n",
        "test_predictions_encoded = imbalance_pipeline.predict(X_test_preprocessed)\n",
        "df_test['ActivityType'] = label_encoder.inverse_transform(test_predictions_encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# xgb"
      ],
      "metadata": {
        "id": "rYZwGVUQYhtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'ActivityType' is your target column\n",
        "X = df.drop('ActivityType', axis=1)  # Features\n",
        "y = df['ActivityType']  # Target\n",
        "\n",
        "# Define preprocessing for numeric and categorical features\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Preprocess the features\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBClassifier with some predefined parameters\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', n_estimators=100, learning_rate=0.1, max_depth=4, subsample=0.8, colsample_bytree=0.8)\n",
        "\n",
        "# Enhanced imbalance handling using a combined SMOTE and RandomUnderSampler approach\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "under = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "steps = [('smote', smote), ('under', under), ('model', xgb_clf)]\n",
        "pipeline = ImbPipeline(steps=steps)\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predictions with the trained model\n",
        "y_pred_train = pipeline.predict(X_train)\n",
        "y_pred_val = pipeline.predict(X_val)\n",
        "\n",
        "# Evaluation with the trained model\n",
        "print(f'Train Precision: {precision_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Precision: {precision_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train Recall: {recall_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Recall: {recall_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train F1 Score: {f1_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation F1 Score: {f1_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, y_pred_train)}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val)}')\n",
        "print('\\nValidation Classification Report:\\n', classification_report(y_val_encoded, y_pred_val))\n",
        "\n",
        "\n",
        "# Assuming df_test is your test DataFrame and it has been processed similarly to how X_train was processed\n",
        "X_test = df_test\n",
        "X_test_preprocessed = preprocessor.transform(X_test)  # Use the preprocessor to transform the test data\n",
        "\n",
        "# Predict the 'ActivityType' using the trained pipeline\n",
        "y_test_pred_encoded = pipeline.predict(X_test_preprocessed)\n",
        "\n",
        "# Decode the predicted labels back to original labels\n",
        "y_test_pred = label_encoder.inverse_transform(y_test_pred_encoded)\n",
        "\n",
        "# Create the output DataFrame\n",
        "output_df = pd.DataFrame({\n",
        "    'RecordID': df_test['RecordID'],\n",
        "    'ActivityType': y_test_pred  # Adjust column name as per your requirement\n",
        "})\n",
        "\n",
        "# Save the output DataFrame to a CSV file\n",
        "output_df.to_csv('/content/drive/MyDrive/ccac_data/xgb_2.csv', index=False)  # Adjust the path as per your environment\n",
        "\n",
        "print(output_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbtdcwCbAfgL",
        "outputId": "b6fb41bb-3b78-437e-daab-1744a8c938bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Precision: 0.638149439633978\n",
            "Validation Precision: 0.6327864376774649\n",
            "Train Recall: 0.6833279831690331\n",
            "Validation Recall: 0.6296562108075059\n",
            "Train F1 Score: 0.6047858836320931\n",
            "Validation F1 Score: 0.5527872748485475\n",
            "Train Accuracy: 0.976828787498656\n",
            "Validation Accuracy: 0.9763452164771098\n",
            "\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.54      0.38       142\n",
            "           1       1.00      1.00      1.00     38237\n",
            "           2       0.67      0.11      0.18        19\n",
            "           3       0.99      0.79      0.88      2612\n",
            "           4       0.19      0.85      0.31       171\n",
            "           5       0.65      0.49      0.56       671\n",
            "\n",
            "    accuracy                           0.98     41852\n",
            "   macro avg       0.63      0.63      0.55     41852\n",
            "weighted avg       0.99      0.98      0.98     41852\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'ActivityType' is your target column\n",
        "X = df.drop('ActivityType', axis=1)  # Features\n",
        "y = df['ActivityType']  # Target\n",
        "\n",
        "# Define preprocessing for numeric and categorical features\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Initialize XGBClassifier with some predefined parameters\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Enhanced imbalance handling using a combined SMOTE and RandomUnderSampler approach\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "under = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "steps = [('preprocessor', preprocessor), ('smote', smote), ('under', under), ('model', xgb_clf)]\n",
        "pipeline = ImbPipeline(steps=steps)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__learning_rate': [0.01, 0.1],\n",
        "    'model__max_depth': [3, 4, 5],\n",
        "    'model__subsample': [0.7, 0.8],\n",
        "    'model__colsample_bytree': [0.7, 0.8],\n",
        "}\n",
        "\n",
        "# Setup the grid search\n",
        "grid_search = GridSearchCV(pipeline, param_grid=parameters, cv=3, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X, y_encoded)\n",
        "\n",
        "# Best parameter set\n",
        "print(f'Best parameters found: {grid_search.best_params_}')\n",
        "\n",
        "# Use the best estimator for further predictions\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Predictions with the trained model (on a separate test set, if available)\n",
        "# Note: Ensure you have a separate test set or split your data accordingly\n",
        "\n",
        "# Example to demonstrate how you would perform predictions with the best estimator\n",
        "# y_test_pred_encoded = best_estimator.predict(X_test_preprocessed)\n",
        "# y_test_pred = label_encoder.inverse_transform(y_test_pred_encoded)\n",
        "# Proceed with evaluating the model as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJtv1cfUYwg4",
        "outputId": "34db422a-d7e2-40dd-ef9e-b1c5d2616f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.6min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.5min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.5min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.6min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.5min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.5min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 3.1min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 3.1min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 3.0min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.8min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.9min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.9min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.9min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.9min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.9min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.9min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.9min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.5min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.5min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.5min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.5min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 4.2min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 4.2min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 4.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 4.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 4.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 4.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 2.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 2.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.4min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.0min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 2.9min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.0min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.0min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 2.9min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.0min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.1min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.0min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.1min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.1min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 3.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 3.6min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 3.6min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 3.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 3.7min\n",
            "[CV] END model__colsample_bytree=0.7, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 3.7min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.6min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.6min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.6min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.6min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 2.7min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 2.9min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 2.9min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.7min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.9min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.9min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 2.0min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 2.0min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.9min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 2.2min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 2.0min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.7min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.7min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.4min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.4min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.4min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 4.3min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 4.6min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 4.6min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 4.3min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 4.6min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 4.6min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.4min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time= 1.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.4min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.4min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time= 1.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 2.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.7; total time= 2.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.3min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200, model__subsample=0.8; total time= 2.5min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.7min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.7; total time= 1.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.7min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=100, model__subsample=0.8; total time= 1.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.0min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.2min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.7; total time= 3.2min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.0min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.1min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=4, model__n_estimators=200, model__subsample=0.8; total time= 3.1min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.1min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.2min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.7; total time= 2.2min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.1min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.2min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time= 2.2min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 3.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 3.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.7; total time= 3.9min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 3.7min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 3.8min\n",
            "[CV] END model__colsample_bytree=0.8, model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 3.8min\n",
            "Best parameters found: {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.1, 'model__max_depth': 5, 'model__n_estimators': 200, 'model__subsample': 0.7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model on the training set\n",
        "best_estimator.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred_encoded = best_estimator.predict(X_val)\n",
        "\n",
        "# Convert encoded labels back to original labels for reporting\n",
        "y_val_pred = label_encoder.inverse_transform(y_val_pred_encoded)\n",
        "y_val_original = label_encoder.inverse_transform(y_val)\n",
        "\n",
        "# Calculate evaluation metrics for the validation set\n",
        "print(f'Precision: {precision_score(y_val, y_val_pred_encoded, average=\"macro\")}')\n",
        "print(f'Recall: {recall_score(y_val, y_val_pred_encoded, average=\"macro\")}')\n",
        "print(f'F1 Score: {f1_score(y_val, y_val_pred_encoded, average=\"macro\")}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val, y_val_pred_encoded)}')\n",
        "print(classification_report(y_val_original, y_val_pred))\n",
        "\n",
        "# Since y_train is used for fitting, you can't directly get a train accuracy without predicting it again,\n",
        "# which is redundant for this scenario. Usually, validation metrics are sufficient to understand model performance.\n"
      ],
      "metadata": {
        "id": "Kx0XkRHYbCV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rf"
      ],
      "metadata": {
        "id": "CNnydDlUYl25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'ActivityType' is your target column\n",
        "X = df.drop('ActivityType', axis=1)  # Features\n",
        "y = df['ActivityType']  # Target\n",
        "\n",
        "# Define preprocessing for numeric and categorical features\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Preprocess the features\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute class weights for imbalance handling\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Initialize and train the RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=class_weights_dict)\n",
        "rf_clf.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on training and validation sets\n",
        "rf_pred_train = rf_clf.predict(X_train)\n",
        "rf_pred_val = rf_clf.predict(X_val)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(f'Train Precision: {precision_score(y_train_encoded, rf_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Precision: {precision_score(y_val_encoded, rf_pred_val, average=\"macro\")}')\n",
        "print(f'Train Recall: {recall_score(y_train_encoded, rf_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Recall: {recall_score(y_val_encoded, rf_pred_val, average=\"macro\")}')\n",
        "print(f'Train F1 Score: {f1_score(y_train_encoded, rf_pred_train, average=\"macro\")}')\n",
        "print(f'Validation F1 Score: {f1_score(y_val_encoded, rf_pred_val, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, rf_pred_train)}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, rf_pred_val)}')\n",
        "print('\\nValidation Classification Report:\\n', classification_report(y_val_encoded, rf_pred_val))\n",
        "\n",
        "# Validation Classification Report:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.29      0.16      0.21       142\n",
        "#            1       1.00      1.00      1.00     38237\n",
        "#            2       0.50      0.11      0.17        19\n",
        "#            3       0.89      0.95      0.92      2612\n",
        "#            4       0.21      0.05      0.08       171\n",
        "#            5       0.65      0.70      0.67       671\n",
        "\n",
        "#     accuracy                           0.98     41852\n",
        "#    macro avg       0.59      0.49      0.51     41852\n",
        "# weighted avg       0.98      0.98      0.98     41852"
      ],
      "metadata": {
        "id": "y7IZbscPM9mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# logistic"
      ],
      "metadata": {
        "id": "0vgbE0bOYo4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'ActivityType' is your target column\n",
        "X = df.drop('ActivityType', axis=1)  # Features\n",
        "y = df['ActivityType']  # Target\n",
        "\n",
        "# Define preprocessing for numeric and categorical features\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Preprocess the features\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize LogisticRegression with some predefined parameters\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Enhanced imbalance handling using a combined SMOTE and RandomUnderSampler approach\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "under = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "steps = [('smote', smote), ('under', under), ('model', log_reg)]\n",
        "pipeline = ImbPipeline(steps=steps)\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predictions with the trained model\n",
        "y_pred_train = pipeline.predict(X_train)\n",
        "y_pred_val = pipeline.predict(X_val)\n",
        "\n",
        "# Evaluation with the trained model\n",
        "print(f'Train Precision: {precision_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Precision: {precision_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train Recall: {recall_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Recall: {recall_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train F1 Score: {f1_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation F1 Score: {f1_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, y_pred_train)}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val)}')\n",
        "print('\\nValidation Classification Report:\\n', classification_report(y_val_encoded, y_pred_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpOCnjsOMm00",
        "outputId": "a118f9c7-5497-4363-8f91-6776dfe8376c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Precision: 0.6578550722710735\n",
            "Validation Precision: 0.5612309151110478\n",
            "Train Recall: 0.8551535980215063\n",
            "Validation Recall: 0.6276773013644936\n",
            "Train F1 Score: 0.6984487825875138\n",
            "Validation F1 Score: 0.5721700442716308\n",
            "Train Accuracy: 0.9885129565248557\n",
            "Validation Accuracy: 0.9832982892095957\n",
            "\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.65      0.43       142\n",
            "           1       1.00      1.00      1.00     38237\n",
            "           2       0.03      0.16      0.05        19\n",
            "           3       0.97      0.89      0.93      2612\n",
            "           4       0.29      0.46      0.35       171\n",
            "           5       0.77      0.60      0.68       671\n",
            "\n",
            "    accuracy                           0.98     41852\n",
            "   macro avg       0.56      0.63      0.57     41852\n",
            "weighted avg       0.99      0.98      0.99     41852\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Load your dataset\n",
        "# df = pd.read_csv('path_to_your_dataset.csv')\n",
        "\n",
        "# Define preprocessing for numeric and categorical features\n",
        "categorical_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['ActivityType'])\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(\n",
        "    df.drop('ActivityType', axis=1),  # Features\n",
        "    y_encoded,  # Encoded target\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize LogisticRegression with balanced class weights and the 'multinomial' option\n",
        "log_reg = LogisticRegression(max_iter=1000, solver='saga', multi_class='multinomial', class_weight='balanced', random_state=42)\n",
        "\n",
        "# Enhanced imbalance handling using a combined SMOTE and RandomUnderSampler approach\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=5, n_jobs=1)  # Adjust k_neighbors if needed\n",
        "under = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "steps = [\n",
        "    ('preprocessor', preprocessor),  # Preprocess the data\n",
        "    ('smote', smote),  # SMOTE for oversampling\n",
        "    ('under', under),  # RandomUnderSampler for undersampling\n",
        "    ('model', log_reg)  # Logistic Regression model\n",
        "]\n",
        "pipeline = ImbPipeline(steps=steps)\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predictions with the trained model\n",
        "y_pred_train = pipeline.predict(X_train)\n",
        "y_pred_val = pipeline.predict(X_val)\n",
        "\n",
        "# Evaluation with the trained model\n",
        "print(f'Train Precision: {precision_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Precision: {precision_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train Recall: {recall_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Recall: {recall_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train F1 Score: {f1_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation F1 Score: {f1_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, y_pred_train)}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val)}')\n",
        "print('\\nValidation Classification Report:\\n', classification_report(y_val_encoded, y_pred_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p33E1M4xdb18",
        "outputId": "e09b7feb-13ab-43b8-f7ec-b2df63ca7534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Precision: 0.6554909649581192\n",
            "Validation Precision: 0.560800132751079\n",
            "Train Recall: 0.8541698929490207\n",
            "Validation Recall: 0.6262058715065237\n",
            "Train F1 Score: 0.6941890537952454\n",
            "Validation F1 Score: 0.5708202921505713\n",
            "Train Accuracy: 0.9883755659892716\n",
            "Validation Accuracy: 0.9832266080474051\n",
            "\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.65      0.42       142\n",
            "           1       1.00      1.00      1.00     38237\n",
            "           2       0.03      0.16      0.04        19\n",
            "           3       0.97      0.89      0.93      2612\n",
            "           4       0.29      0.46      0.35       171\n",
            "           5       0.77      0.60      0.67       671\n",
            "\n",
            "    accuracy                           0.98     41852\n",
            "   macro avg       0.56      0.63      0.57     41852\n",
            "weighted avg       0.99      0.98      0.99     41852\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'ActivityType' is your target column\n",
        "\n",
        "# Define preprocessing for numeric and categorical features\n",
        "categorical_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['ActivityType'])\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(\n",
        "    df.drop('ActivityType', axis=1),  # Features\n",
        "    y_encoded,  # Encoded target\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('under', RandomUnderSampler(random_state=42)),\n",
        "    ('model', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'model__C': [0.01, 0.1, 1, 10],\n",
        "    'model__solver': ['liblinear', 'saga'],\n",
        "    'smote__k_neighbors': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model using GridSearchCV\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Predictions with the best model from grid search\n",
        "y_pred_train = grid_search.predict(X_train)\n",
        "y_pred_val = grid_search.predict(X_val)\n",
        "\n",
        "# Evaluation with the best model\n",
        "print(f'Train Precision: {precision_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Precision: {precision_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train Recall: {recall_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation Recall: {recall_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train F1 Score: {f1_score(y_train_encoded, y_pred_train, average=\"macro\")}')\n",
        "print(f'Validation F1 Score: {f1_score(y_val_encoded, y_pred_val, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, y_pred_train)}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val)}')\n",
        "print('\\nValidation Classification Report:\\n', classification_report(y_val_encoded, y_pred_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWbQ4TNffkVj",
        "outputId": "d231a8b2-2d01-4a8c-8abc-356137b1009f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'model__C': 1, 'model__solver': 'saga', 'smote__k_neighbors': 7}\n",
            "Train Precision: 0.6574633037759763\n",
            "Validation Precision: 0.5626240029068604\n",
            "Train Recall: 0.8549934175520407\n",
            "Validation Recall: 0.6293971167167508\n",
            "Train F1 Score: 0.6974723980823683\n",
            "Validation F1 Score: 0.5735453342894027\n",
            "Train Accuracy: 0.9884651685124787\n",
            "Validation Accuracy: 0.9833938640925165\n",
            "\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.65      0.42       142\n",
            "           1       1.00      1.00      1.00     38237\n",
            "           2       0.03      0.16      0.05        19\n",
            "           3       0.97      0.89      0.93      2612\n",
            "           4       0.30      0.47      0.36       171\n",
            "           5       0.77      0.61      0.68       671\n",
            "\n",
            "    accuracy                           0.98     41852\n",
            "   macro avg       0.56      0.63      0.57     41852\n",
            "weighted avg       0.99      0.98      0.99     41852\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ensemble 2 with manual cleaning"
      ],
      "metadata": {
        "id": "j0woNBRnW1Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming df is your DataFrame containing the training data and df_test is the DataFrame for predictions\n",
        "\n",
        "# Define preprocessing\n",
        "categorical_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['ActivityType'])\n",
        "X_preprocessed = preprocessor.fit_transform(df.drop('ActivityType', axis=1))\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Count the occurrences of each class in the target variable\n",
        "class_counts = Counter(y_train_encoded)\n",
        "sampling_strategy_smote = {class_label: int(count * 1.5) for class_label, count in class_counts.items() if count / len(y_train_encoded) < 0.91}\n",
        "sampling_strategy_under = {class_label: int(count * 0.5) for class_label, count in class_counts.items() if count / len(y_train_encoded) >= 0.91}\n",
        "\n",
        "# Compute class weights for imbalance handling\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Initialize estimators\n",
        "estimator1 = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_leaf=1, min_samples_split=2, random_state=42, class_weight=class_weights_dict, n_jobs=-1 )\n",
        "estimator2 = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "estimator3 = XGBClassifier(random_state=42, use_label_encoder=False, colsample_bytree=0.7,\n",
        "                           learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7, eval_metric='mlogloss', n_jobs=-1 )\n",
        "estimator4 = LogisticRegression(max_iter=1000, random_state=42)  # Adding LogisticRegression\n",
        "\n",
        "# Create a VotingClassifier with soft voting\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', estimator1),\n",
        "    ('gb', estimator2),\n",
        "    ('xgb', estimator3),\n",
        "    ('lr', estimator4)],  # Add LogisticRegression to the voting classifier\n",
        "    voting='soft')\n",
        "\n",
        "\n",
        "# Define the imbalance pipeline\n",
        "imbalance_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state=42, sampling_strategy=sampling_strategy_smote)),\n",
        "    ('under', RandomUnderSampler(random_state=42, sampling_strategy=sampling_strategy_under)),\n",
        "    ('voting', voting_clf)\n",
        "])\n",
        "\n",
        "# Train the model on the training set\n",
        "imbalance_pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_val_encoded = imbalance_pipeline.predict(X_val)\n",
        "\n",
        "# Print evaluation metrics for the validation set\n",
        "print(f'Precision: {precision_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Recall: {recall_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'F1 Score: {f1_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, imbalance_pipeline.predict(X_train))}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val_encoded)}')\n",
        "print(classification_report(y_val_encoded, y_pred_val_encoded))\n",
        "\n",
        "# Predict on the test data using the trained pipeline\n",
        "X_test_preprocessed = preprocessor.transform(df_test)\n",
        "test_predictions_encoded = imbalance_pipeline.predict(X_test_preprocessed)\n",
        "df_test['ActivityType'] = label_encoder.inverse_transform(test_predictions_encoded)\n",
        "\n",
        "# Output the prediction in the desired format\n",
        "output_df = df_test[['RecordID', 'ActivityType']]\n",
        "output_df.to_csv('/content/drive/MyDrive/ccac_data/ensemble_predictions_cleaned_gs_lr.csv', index=False)\n",
        "print(output_df.head())\n"
      ],
      "metadata": {
        "id": "Jf4hx2rQW6dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Model"
      ],
      "metadata": {
        "id": "EQnpR4nYi1pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming df is your DataFrame containing the training data and df_test is the DataFrame for predictions\n",
        "\n",
        "# Define preprocessing\n",
        "categorical_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['ActivityType'])\n",
        "X_preprocessed = preprocessor.fit_transform(df.drop('ActivityType', axis=1))\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Count the occurrences of each class in the target variable\n",
        "class_counts = Counter(y_train_encoded)\n",
        "sampling_strategy_smote = {class_label: int(count * 1.5) for class_label, count in class_counts.items() if count / len(y_train_encoded) < 0.91}\n",
        "sampling_strategy_under = {class_label: int(count * 0.5) for class_label, count in class_counts.items() if count / len(y_train_encoded) >= 0.91}\n",
        "\n",
        "# Compute class weights for imbalance handling\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Initialize estimators\n",
        "estimator1 = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_leaf=1, min_samples_split=2, random_state=42, class_weight=class_weights_dict, n_jobs=-1 )\n",
        "estimator2 = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "estimator3 = XGBClassifier(random_state=42, use_label_encoder=False, colsample_bytree=0.7,\n",
        "                           learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7, eval_metric='mlogloss', n_jobs=-1 )\n",
        "estimator4 = LogisticRegression(max_iter=1000, random_state=42)  # Adding LogisticRegression\n",
        "\n",
        "# Create a VotingClassifier with soft voting\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', estimator1),\n",
        "    ('gb', estimator2),\n",
        "    ('xgb', estimator3),\n",
        "    ('lr', estimator4)],  # Add LogisticRegression to the voting classifier\n",
        "    voting='soft')\n",
        "\n",
        "\n",
        "# Define the imbalance pipeline\n",
        "imbalance_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state=42, sampling_strategy=sampling_strategy_smote)),\n",
        "    ('under', RandomUnderSampler(random_state=42, sampling_strategy=sampling_strategy_under)),\n",
        "    ('voting', voting_clf)\n",
        "])\n",
        "\n",
        "# Train the model on the training set\n",
        "imbalance_pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_val_encoded = imbalance_pipeline.predict(X_val)\n",
        "\n",
        "# Print evaluation metrics for the validation set\n",
        "print(f'Precision: {precision_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Recall: {recall_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'F1 Score: {f1_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, imbalance_pipeline.predict(X_train))}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val_encoded)}')\n",
        "print(classification_report(y_val_encoded, y_pred_val_encoded))\n",
        "\n",
        "# Predict on the test data using the trained pipeline\n",
        "X_test_preprocessed = preprocessor.transform(df_test)\n",
        "test_predictions_encoded = imbalance_pipeline.predict(X_test_preprocessed)\n",
        "df_test['ActivityType'] = label_encoder.inverse_transform(test_predictions_encoded)\n",
        "\n",
        "# Output the prediction in the desired format\n",
        "output_df = df_test[['RecordID', 'ActivityType']]\n",
        "output_df.to_csv('/content/drive/MyDrive/ccac_data/ensemble_predictions_cleaned_gs_lr.csv', index=False)\n",
        "print(output_df.head())\n"
      ],
      "metadata": {
        "id": "vs00Dc8kXtAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "a7f9ee7e-f753-4c38-d27c-d6559f2449a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6352314079658715\n",
            "Recall: 0.5178443845867483\n",
            "F1 Score: 0.5339687439380824\n",
            "Train Accuracy: 0.9943132265271256\n",
            "Validation Accuracy: 0.9877903087068718\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.19      0.28       142\n",
            "           1       1.00      1.00      1.00     38237\n",
            "           2       0.25      0.05      0.09        19\n",
            "           3       0.92      0.96      0.94      2612\n",
            "           4       0.41      0.09      0.14       171\n",
            "           5       0.70      0.81      0.75       671\n",
            "\n",
            "    accuracy                           0.99     41852\n",
            "   macro avg       0.64      0.52      0.53     41852\n",
            "weighted avg       0.99      0.99      0.99     41852\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: '/kaggle/working'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-89b011b85f90>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Output the prediction in the desired format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0moutput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RecordID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ActivityType'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0moutput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/ensemble_predictions_cleaned_gs_lr.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3718\u001b[0m         )\n\u001b[1;32m   3719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3720\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         )\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/kaggle/working'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming df is your DataFrame containing the training data and df_test is the DataFrame for predictions\n",
        "\n",
        "# Define preprocessing\n",
        "categorical_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = df.drop('ActivityType', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
        "    ('num', StandardScaler(), numeric_columns)\n",
        "], remainder='passthrough')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['ActivityType'])\n",
        "X_preprocessed = preprocessor.fit_transform(df.drop('ActivityType', axis=1))\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_preprocessed, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Count the occurrences of each class in the target variable\n",
        "class_counts = Counter(y_train_encoded)\n",
        "sampling_strategy_smote = {class_label: int(count * 1.5) for class_label, count in class_counts.items() if count / len(y_train_encoded) < 0.91}\n",
        "sampling_strategy_under = {class_label: int(count * 0.5) for class_label, count in class_counts.items() if count / len(y_train_encoded) >= 0.91}\n",
        "\n",
        "# Compute class weights for imbalance handling\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Initialize estimators\n",
        "estimator1 = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_leaf=1, min_samples_split=2, random_state=42, class_weight=class_weights_dict, n_jobs=-1 )\n",
        "estimator2 = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "estimator3 = XGBClassifier(random_state=42, use_label_encoder=False, colsample_bytree=0.7,\n",
        "                           learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7, eval_metric='mlogloss', n_jobs=-1 )\n",
        "estimator4 = LogisticRegression(max_iter=1000, random_state=42)  # Adding LogisticRegression\n",
        "\n",
        "# Create a VotingClassifier with soft voting\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', estimator1),\n",
        "    ('gb', estimator2),\n",
        "    ('xgb', estimator3),\n",
        "    ('lr', estimator4)],  # Add LogisticRegression to the voting classifier\n",
        "    voting='soft',weights=[1, 1, 1,2])\n",
        "\n",
        "\n",
        "# Define the imbalance pipeline\n",
        "imbalance_pipeline = ImbPipeline(steps=[\n",
        "    ('smote', SMOTE(random_state=42, sampling_strategy=sampling_strategy_smote)),\n",
        "    ('under', RandomUnderSampler(random_state=42, sampling_strategy=sampling_strategy_under)),\n",
        "    ('voting', voting_clf,)\n",
        "])\n",
        "\n",
        "# Train the model on the training set\n",
        "imbalance_pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_val_encoded = imbalance_pipeline.predict(X_val)\n",
        "\n",
        "# Print evaluation metrics for the validation set\n",
        "print(f'Precision: {precision_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Recall: {recall_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'F1 Score: {f1_score(y_val_encoded, y_pred_val_encoded, average=\"macro\")}')\n",
        "print(f'Train Accuracy: {accuracy_score(y_train_encoded, imbalance_pipeline.predict(X_train))}')\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val_encoded, y_pred_val_encoded)}')\n",
        "print(classification_report(y_val_encoded, y_pred_val_encoded))\n",
        "\n",
        "# Predict on the test data using the trained pipeline\n",
        "X_test_preprocessed = preprocessor.transform(df_test)\n",
        "test_predictions_encoded = imbalance_pipeline.predict(X_test_preprocessed)\n",
        "df_test['ActivityType'] = label_encoder.inverse_transform(test_predictions_encoded)\n",
        "\n",
        "# Output the prediction in the desired format\n",
        "output_df = df_test[['RecordID', 'ActivityType']]\n",
        "output_df.to_csv('/content/drive/MyDrive/ccac_data/ensemble_best_exp1.csv', index=False)\n",
        "print(output_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpoet3tRYLmP",
        "outputId": "2eb392eb-8e41-40d5-b971-6518ae1f0ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6950321658053565\n",
            "Recall: 0.525721131346141\n",
            "F1 Score: 0.5464248051206169\n",
            "Train Accuracy: 0.9941877829946358\n",
            "Validation Accuracy: 0.9882203956800153\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.18      0.27       142\n",
            "           1       1.00      1.00      1.00     38237\n",
            "           2       0.50      0.05      0.10        19\n",
            "           3       0.92      0.96      0.94      2612\n",
            "           4       0.46      0.13      0.21       171\n",
            "           5       0.71      0.83      0.76       671\n",
            "\n",
            "    accuracy                           0.99     41852\n",
            "   macro avg       0.70      0.53      0.55     41852\n",
            "weighted avg       0.99      0.99      0.99     41852\n",
            "\n",
            "   RecordID ActivityType\n",
            "0     26355  No Activity\n",
            "1     26641  No Activity\n",
            "2     26836  No Activity\n",
            "3     26900  No Activity\n",
            "4     26948  No Activity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w5izmRSL6tBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}